
### Implementations
 
- [Feed forward Neural Network from scratch](https://github.com/srkds/Neural-networks-GYM/blob/main/Improve-Deep-NN/NN/src/nn.py), that includes forward pass, backwardpass, Activation Functions(ReLU, Sigmoid, Softmax), Loss functions(binary cross entropy, cross entropy), and regularizations like Dropout, and L2.
- Try your self training any multiclass classification task, start with [training MNIST hand written digit classification](https://github.com/srkds/Neural-networks-GYM/tree/main/Improve-Deep-NN/NN/mnist)

### Papers

- Dropout : [Dropout: A Simple Way to Prevent Neural Networks from Overfitting](https://www.cs.toronto.edu/~rsalakhu/papers/srivastava14a.pdf) _Srivastava et al.(2014)_
- Batch Norm : [Batch Normalization: Accelerating Deep Network Training by Reducing Internal Covariate Shift](https://arxiv.org/abs/1502.03167) _Ioffe et al.(2015)_
- Adam Optimizer: [Adam: A Method for Stochastic Optimization](https://arxiv.org/abs/1412.6980) _Kingma et al.(2017)_
- Backpropagation: [Learning representations by back-propagating errors](https://www.iro.umontreal.ca/~vincentp/ift3395/lectures/backprop_old.pdf)
